**é©šæ„•ã®äº‹å®Ÿç™ºè¦‹ï¼ï¼** ğŸ˜±

## ğŸš¨ T001åˆ†æçµæœ - é‡å¤§ãªå•é¡Œã‚’ç™ºè¦‹

### ğŸ“Š ç¾çŠ¶ã®æ•°å­—

- **ç·ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°**: 2,428 (ç›®æ¨™é€šã‚Š)
- **å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: 39/40 (1ãƒ•ã‚¡ã‚¤ãƒ«å¤±æ•—)
- **å‹ä¸€è‡´ç‡**: **ã‚ãšã‹42.7%** â—
- **å‹ä¸ä¸€è‡´æ•°**: **1,392ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰** â—â—

### ğŸ” ä¸»è¦ãªå•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³

**å…¨ã¦ã®ä¸ä¸€è‡´ãŒã€Œæ¨å®šå‹ â†’ TEXTã€ã«ãªã£ã¦ã„ã‚‹ï¼**

1. **REAL â†’ TEXT**: 1,025ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (æœ€å¤§ã®å•é¡Œ)
2. **ï¼ˆä¸æ˜å½¢å¼ï¼‰ â†’ TEXT**: 138ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
3. **DATETIME â†’ TEXT**: 118ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
4. **INTEGER â†’ TEXT**: 111ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

### ğŸ¯ æ ¹æœ¬åŸå› ã®ä»®èª¬

**Actual_TypeãŒå…¨ã¦TEXTã«ãªã£ã¦ã„ã‚‹ = è‡ªå‹•æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ã«é‡å¤§ãªå•é¡Œ**

ã“ã‚Œã¯æƒ³å®šã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«æ·±åˆ»ãªçŠ¶æ³ã§ã™ã€‚å‹æ¨å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è‡ªä½“ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚

## ğŸš€ ç·Šæ€¥å¯¾å¿œè¨ˆç”»

**T001ã‚’è©³ç´°åˆ†æãƒ„ãƒ¼ãƒ«ã«å¤‰æ›´**ã—ã¦ã€æ ¹æœ¬åŸå› ã‚’ç‰¹å®šã—ã¾ã—ã‚‡ã†ï¼š

S

Create text file

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `content`: `#!/usr/bin/env python3 \"\"\" T001: compare_report.csvè©³ç´°åˆ†æãƒ„ãƒ¼ãƒ« Typeæ¨å®šã®å•é¡Œã‚’è©³ç´°ã«èª¿æŸ»ã—ã€ä¿®æ­£æ–¹é‡ã‚’æ±ºå®šã™ã‚‹ \"\"\" import pandas as pd import os import numpy as np from config import OUTPUT_DIR def analyze_compare_report(): \"\"\"compare_report.csvã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ\"\"\" print(\"ğŸ” T001: compare_report.csv è©³ç´°åˆ†æé–‹å§‹\") print(\"=\" * 60) # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ compare_file = os.path.join(OUTPUT_DIR, 'compare_report.csv') if not os.path.exists(compare_file): print(f\"âŒ compare_report.csvãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {compare_file}\") return df = pd.read_csv(compare_file, encoding='utf-8') # åŸºæœ¬æƒ…å ± print(f\"ğŸ“Š åŸºæœ¬æƒ…å ±\") print(f\" ç·ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(df):,}\") print(f\" å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {df['File'].nunique()}\") print(f\" ãƒ¦ãƒ‹ãƒ¼ã‚¯åˆ—åæ•°: {df['Column'].nunique()}\") print() # é‡å¤§ãªå•é¡Œï¼šActual_Typeã®èª¿æŸ» print(f\"ğŸš¨ é‡å¤§ãªå•é¡Œç™ºè¦‹\") print(f\" Actual_Type ã®åˆ†å¸ƒ:\") actual_types = df['Actual_Type'].value_counts() for dtype, count in actual_types.items(): print(f\" {dtype}: {count:,} ({count/len(df)*100:.1f}%)\") print() if len(actual_types) == 1 and 'TEXT' in actual_types.index: print(\" âš ï¸ å…¨ã¦ã®Actual_TypeãŒTEXTã«ãªã£ã¦ã„ã¾ã™ï¼\") print(\" â†’ å‹æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ã«è‡´å‘½çš„ãªå•é¡ŒãŒã‚ã‚Šã¾ã™\") print() # å‹ä¸€è‡´ç‡ã®è©³ç´° print(f\"ğŸ“ˆ å‹ä¸€è‡´çŠ¶æ³\") match_dist = df['Match'].value_counts() total = len(df) match_rate = (df['Match'] == 'â—‹').sum() / total * 100 print(f\" ä¸€è‡´: {match_dist.get('â—‹', 0):,} ({match_dist.get('â—‹', 0)/total*100:.1f}%)\") print(f\" ä¸ä¸€è‡´: {match_dist.get('Ã—', 0):,} ({match_dist.get('Ã—', 0)/total*100:.1f}%)\") print(f\" ä¸€è‡´ç‡: {match_rate:.1f}%\") print() # æ¨å®šå‹ã®åˆ†å¸ƒ print(f\"ğŸ§  æ¨å®šå‹ã®åˆ†å¸ƒ\") inferred_dist = df['Inferred_Type'].value_counts() for dtype, count in inferred_dist.items(): print(f\" {dtype}: {count:,} ({count/len(df)*100:.1f}%)\") print() # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥åˆ†æ print(f\"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥åˆ†æï¼ˆä¸Šä½10ä»¶ï¼‰\") file_analysis = df.groupby('File').agg({ 'Column': 'count', 'Match': lambda x: (x == 'â—‹').sum(), 'Encoding': lambda x: x.iloc[0], 'Delimiter': lambda x: x.iloc[0] }).rename(columns={'Column': 'total_fields', 'Match': 'matched_fields'}) file_analysis['match_rate'] = file_analysis['matched_fields'] / file_analysis['total_fields'] * 100 file_analysis = file_analysis.sort_values('match_rate') print(\" ãƒ¯ãƒ¼ã‚¹ãƒˆ10ãƒ•ã‚¡ã‚¤ãƒ« (ä¸€è‡´ç‡é †):\") for file, row in file_analysis.head(10).iterrows(): print(f\" {file[:30]:30} | {row['matched_fields']:3}/{row['total_fields']:3} ({row['match_rate']:5.1f}%) | {row['Encoding']} | {row['Delimiter']}\") print() # å‹ä¸ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´° print(f\"ğŸ”„ å‹ä¸ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°\") mismatch_df = df[df['Match'] != 'â—‹'] if len(mismatch_df) > 0: patterns = mismatch_df.groupby(['Inferred_Type', 'Actual_Type']).size().sort_values(ascending=False) print(\" ä¸»è¦ãªä¸ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³:\") for (inferred, actual), count in patterns.head(15).items(): print(f\" {inferred:12} â†’ {actual:12}: {count:4,} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ({count/len(mismatch_df)*100:5.1f}%)\") print() # SAPãƒ‡ãƒ¼ã‚¿ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º print(f\"ğŸ¯ SAPãƒ‡ãƒ¼ã‚¿ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º\") # 0ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¯èƒ½æ€§ zero_padding_candidates = df[ (df['Column'].str.contains('CODE|CD|NO|NUM', case=False, na=False)) & (df['Inferred_Type'] == 'TEXT') ] print(f\" 0ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å€™è£œ: {len(zero_padding_candidates)} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\") # å¾Œã‚ãƒã‚¤ãƒŠã‚¹å¯èƒ½æ€§ï¼ˆæ•°å€¤ã ãŒæ¨å®šãŒTEXTã®ã‚‚ã®ï¼‰ minus_candidates = df[ (df['Inferred_Type'] != 'TEXT') & (df['Actual_Type'] == 'TEXT') ] print(f\" å¾Œã‚ãƒã‚¤ãƒŠã‚¹å€™è£œ: {len(minus_candidates)} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\") print() # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»åŒºåˆ‡ã‚Šæ–‡å­—åˆ†æ print(f\"ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ†æ\") print(\" ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†å¸ƒ:\") enc_dist = df['Encoding'].value_counts() for enc, count in enc_dist.items(): unique_files = df[df['Encoding'] == enc]['File'].nunique() print(f\" {enc:10}: {count:4,} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ({unique_files} ãƒ•ã‚¡ã‚¤ãƒ«)\") print(\" åŒºåˆ‡ã‚Šæ–‡å­—åˆ†å¸ƒ:\") delim_dist = df['Delimiter'].value_counts() for delim, count in delim_dist.items(): unique_files = df[df['Delimiter'] == delim]['File'].nunique() delim_display = repr(delim) if delim in ['\ ', '\ ', '\\r'] else delim print(f\" {delim_display:10}: {count:4,} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ({unique_files} ãƒ•ã‚¡ã‚¤ãƒ«)\") print() # é‡è¤‡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã®æ¤œå‡º print(f\"ğŸ” é‡è¤‡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã®æ¤œå‡º\") column_counts = df['Column'].value_counts() duplicates = column_counts[column_counts > 1] print(f\" é‡è¤‡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åæ•°: {len(duplicates)}\") if len(duplicates) > 0: print(\" ä¸Šä½é‡è¤‡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:\") for col, count in duplicates.head(10).items(): files_with_col = df[df['Column'] == col]['File'].nunique() print(f\" {col:30}: {count:3} å›å‡ºç¾ ({files_with_col} ãƒ•ã‚¡ã‚¤ãƒ«)\") print() # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ print(f\"ğŸ“‹ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\") print(\" 1. ğŸš¨ ç·Šæ€¥: Actual_Typeæ¨å®šãƒ­ã‚¸ãƒƒã‚¯ã®ä¿®æ­£\") print(\" â†’ analyzer.py ã®å‹æ¨å®šé–¢æ•°ã‚’ç¢ºèª\") print(\" â†’ å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆ\") print() print(\" 2. ğŸ”„ SAPãƒ‡ãƒ¼ã‚¿ç‰¹æ®Šãƒ«ãƒ¼ãƒ«ã®å®Ÿè£…\") print(f\" â†’ 0ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å€™è£œ: {len(zero_padding_candidates)} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\") print(f\" â†’ å¾Œã‚ãƒã‚¤ãƒŠã‚¹å€™è£œ: {len(minus_candidates)} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\") print() print(\" 3. ğŸ“Š é‡è¤‡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®çµ±åˆè¨ˆç”»\") print(f\" â†’ é‡è¤‡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {len(duplicates)} ç¨®é¡\") print() print(\" 4. ğŸ§ª æ®µéšçš„ä¿®æ­£ã®å®Ÿæ–½\") print(\" â†’ ã¾ãš1ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ†ã‚¹ãƒˆ\") print(\" â†’ æˆåŠŸå¾Œã«å…¨ä½“é©ç”¨\") print(\"\ \" + \"=\" * 60) print(\"ğŸ¯ T001åˆ†æå®Œäº† - æ¬¡ã®ã‚¿ã‚¹ã‚¯: analyzer.py ã®å‹æ¨å®šãƒ­ã‚¸ãƒƒã‚¯èª¿æŸ»\") if __name__ == \"__main__\": analyze_compare_report() `, `relative_path`: `t001_analyzer.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`File created: t001_analyzer.py.`

S

Execute shell command

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `command`: `python t001_analyzer.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`{"stdout":"","return_code":1,"cwd":"C:\\Projects_workspace\\03_python","stderr":"Traceback (most recent call last):\n File \"C:\\Projects_workspace\\03_python\\t001_analyzer.py\", line 159, in <module>\n analyze_compare_report()\n File \"C:\\Projects_workspace\\03_python\\t001_analyzer.py\", line 16, in analyze_compare_report\n print(\"\\U0001f50d T001: compare_report.csv ï¿½Ú×•ï¿½ï¿½ÍŠJï¿½n\")\nUnicodeEncodeError: 'cp932' codec can't encode character '\\U0001f50d' in position 0: illegal multibyte sequence\n"}`

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚çµµæ–‡å­—ã‚’å‰Šé™¤ã—ã¦å†å®Ÿè¡Œã—ã¾ã™ï¼š

S

Replace regex

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```{ `repl`: ``, `regex`: `ğŸ”|ğŸ“Š|ğŸš¨|ğŸ“ˆ|ğŸ§ |ğŸ“|ğŸ”„|ğŸ¯|ğŸ”§|ğŸ“‹|âš ï¸|â†’|âŒ|ğŸ“Š|ğŸ¯`, `relative_path`: `t001_analyzer.py`, `allow_multiple_occurrences`: true }```

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`OK`

S

Execute shell command

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `command`: `python t001_analyzer.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

``{"stdout":" T001: compare_report.csv ï¿½Ú×•ï¿½ï¿½ÍŠJï¿½n\n============================================================\n ï¿½ï¿½{ï¿½ï¿½ï¿½\n ï¿½ï¿½ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½hï¿½ï¿½: 2,428\n ï¿½ï¿½ï¿½ï¿½ï¿½tï¿½@ï¿½Cï¿½ï¿½ï¿½ï¿½: 39\n ï¿½ï¿½ï¿½jï¿½[ï¿½Nï¿½ñ–¼ï¿½: 1822\n\n ï¿½dï¿½ï¿½È–ï¿½è”­ï¿½ï¿½\n Actual_Type ï¿½Ì•ï¿½ï¿½z:\n TEXT: 2,428 (100.0%)\n\n ï¿½Sï¿½Ä‚ï¿½Actual_Typeï¿½ï¿½TEXTï¿½É‚È‚ï¿½ï¿½Ä‚ï¿½ï¿½Ü‚ï¿½ï¿½I\n ï¿½^ï¿½ï¿½ï¿½èƒï¿½Wï¿½bï¿½Nï¿½É’vï¿½ï¿½ï¿½Iï¿½È–ï¿½è‚ªï¿½ï¿½ï¿½ï¿½Ü‚ï¿½\n\n ï¿½^ï¿½ï¿½vï¿½ï¿½\n ï¿½ï¿½v: 1,036 (42.7%)\n ï¿½sï¿½ï¿½v: 1,392 (57.3%)\n ï¿½ï¿½vï¿½ï¿½: 42.7%\n\n ï¿½ï¿½ï¿½ï¿½^ï¿½Ì•ï¿½ï¿½z\n TEXT: 1,036 (42.7%)\n REAL: 1,025 (42.2%)\n ï¿½iï¿½ï¿½ï¿½oï¿½^ï¿½j: 138 (5.7%)\n DATETIME: 118 (4.9%)\n INTEGER: 111 (4.6%)\n\n ï¿½tï¿½@ï¿½Cï¿½ï¿½ï¿½Ê•ï¿½ï¿½Íiï¿½ï¿½ï¿½10ï¿½ï¿½ï¿½j\n ï¿½ï¿½ï¿½[ï¿½Xï¿½g10ï¿½tï¿½@ï¿½Cï¿½ï¿½ (ï¿½ï¿½vï¿½ï¿½ï¿½ï¿½):\n ï¿½ï¿½ï¿½oï¿½ï¿½ï¿½×iï¿½ï¿½ï¿½j_ZPR01201.txt | 0/ 34 ( 0.0%) | cp932 | \t\n zs65_sss.txt | 0/ 35 ( 0.0%) | cp932 | \t\n zs65.txt | 0/ 35 ( 0.0%) | cp932 | \t\n ï¿½ï¿½ï¿½oï¿½ï¿½ï¿½×iï¿½ï¿½ï¿½ï¿½j_ZPR01201.txt | 0/ 34 ( 0.0%) | cp932 | \t\n ZPF01802_ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.TXT | 40/819 ( 4.9%) | cp932 | \t\n ZP173.TXT | 21/ 72 ( 29.2%) | cp932 | \t\n GetSekkeiWBSJisseki.txt | 11/ 25 ( 44.0%) | cp932 | \t\n zp02.txt | 21/ 47 ( 44.7%) | cp932 | \t\n ZP173_MEISAI.TXT | 26/ 50 ( 52.0%) | cp932 | \t\n PP_DL_CSV_ZTBP110.csv | 21/ 40 ( 52.5%) | utf-16 | ,\n\n ï¿½^ï¿½sï¿½ï¿½vï¿½pï¿½^ï¿½[ï¿½ï¿½ï¿½ÌÚï¿½\n ï¿½ï¿½vï¿½È•sï¿½ï¿½vï¿½pï¿½^ï¿½[ï¿½ï¿½:\n REAL TEXT : 1,025 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h ( 73.6%)\n ï¿½iï¿½ï¿½ï¿½oï¿½^ï¿½j TEXT : 138 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h ( 9.9%)\n DATETIME TEXT : 118 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h ( 8.5%)\n INTEGER TEXT : 111 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h ( 8.0%)\n\n SAPï¿½fï¿½[ï¿½^ï¿½ï¿½ï¿½ï¿½pï¿½^ï¿½[ï¿½ï¿½ï¿½ÌŒï¿½ï¿½o\n 0ï¿½pï¿½fï¿½Bï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½: 16 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h\n ï¿½ï¿½ï¿½}ï¿½Cï¿½iï¿½Xï¿½ï¿½ï¿½: 1392 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h\n\n ï¿½tï¿½@ï¿½Cï¿½ï¿½ï¿½`ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n ï¿½Gï¿½ï¿½ï¿½Rï¿½[ï¿½fï¿½Bï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½z:\n cp932 : 2,239 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h (35 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n excel : 103 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h (2 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n utf-16 : 86 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h (2 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½ï¿½Ø‚è•¶ï¿½ï¿½ï¿½ï¿½ï¿½z:\n '\\t' : 2,051 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h (31 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n , : 274 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h (6 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n\n ï¿½dï¿½ï¿½ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½ÌŒï¿½ï¿½o\n ï¿½dï¿½ï¿½ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½ï¿½: 316\n ï¿½ï¿½Êdï¿½ï¿½ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h:\n ï¿½iï¿½ÚƒRï¿½[ï¿½h : 27 ï¿½ï¿½oï¿½ï¿½ (27 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½iï¿½Úƒeï¿½Lï¿½Xï¿½g : 22 ï¿½ï¿½oï¿½ï¿½ (22 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½vï¿½ï¿½ï¿½ï¿½ï¿½g : 21 ï¿½ï¿½oï¿½ï¿½ (21 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n MRPï¿½Ç—ï¿½ï¿½ï¿½ : 18 ï¿½ï¿½oï¿½ï¿½ (18 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½ÛŠÇêŠ : 15 ï¿½ï¿½oï¿½ï¿½ (15 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½iï¿½Úƒ^ï¿½Cï¿½v : 10 ï¿½ï¿½oï¿½ï¿½ (10 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½ó’“`ï¿½[ï¿½Ôï¿½ : 10 ï¿½ï¿½oï¿½ï¿½ (10 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n WBSï¿½vï¿½f : 10 ï¿½ï¿½oï¿½ï¿½ (10 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½ó’–ï¿½ï¿½×”Ôï¿½ : 9 ï¿½ï¿½oï¿½ï¿½ (9 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n ï¿½wï¿½}ï¿½^ï¿½Cï¿½v : 8 ï¿½ï¿½oï¿½ï¿½ (8 ï¿½tï¿½@ï¿½Cï¿½ï¿½)\n\n ï¿½ï¿½ï¿½ï¿½ï¿½Aï¿½Nï¿½Vï¿½ï¿½ï¿½ï¿½\n 1. ï¿½Ù‹}: Actual_Typeï¿½ï¿½ï¿½èƒï¿½Wï¿½bï¿½Nï¿½ÌCï¿½ï¿½\n analyzer.py ï¿½ÌŒ^ï¿½ï¿½ï¿½ï¿½Öï¿½ï¿½ï¿½ï¿½mï¿½F\n ï¿½ï¿½ï¿½Û‚Ìƒfï¿½[ï¿½^ï¿½Tï¿½ï¿½ï¿½vï¿½ï¿½ï¿½Åƒeï¿½Xï¿½g\n\n 2. SAPï¿½fï¿½[ï¿½^ï¿½ï¿½ï¿½êƒ‹ï¿½[ï¿½ï¿½ï¿½Ìï¿½ï¿½ï¿½\n 0ï¿½pï¿½fï¿½Bï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½: 16 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h\n ï¿½ï¿½ï¿½}ï¿½Cï¿½iï¿½Xï¿½ï¿½ï¿½: 1392 ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h\n\n 3. ï¿½dï¿½ï¿½ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½hï¿½Ì“ï¿½ï¿½ï¿½ï¿½vï¿½ï¿½\n ï¿½dï¿½ï¿½ï¿½tï¿½Bï¿½[ï¿½ï¿½ï¿½h: 316 ï¿½ï¿½ï¿½\n\n","return_code":1,"cwd":"C:\\Projects_workspace\\03_python","stderr":"Traceback (most recent call last):\n File \"C:\\Projects_workspace\\03_python\\t001_analyzer.py\", line 159, in <module>\n analyze_compare_report()\n File \"C:\\Projects_workspace\\03_python\\t001_analyzer.py\", line 150, in analyze_compare_report\n print(\" 4. \\U0001f9ea ï¿½iï¿½Kï¿½Iï¿½Cï¿½ï¿½ï¿½Ìï¿½ï¿½{\")\nUnicodeEncodeError: 'cp932' codec can't encode character '\\U0001f9ea' in position 5: illegal multibyte sequence\n"}``

ç´ æ™´ã‚‰ã—ã„æƒ…å ±ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸï¼**T001ã®æ ¸å¿ƒçš„ãªå•é¡ŒãŒæ˜ç¢ºã«ãªã‚Šã¾ã—ãŸ**ã€‚

## ğŸ¯ T001åˆ†æçµæœ - æ±ºå®šçš„äº‹å®Ÿ

### ğŸ’¥ **æ ¹æœ¬åŸå› ç¢ºå®š**

**Actual_TypeãŒ100%å…¨ã¦TEXTã«ãªã£ã¦ã„ã‚‹** = **analyzer.pyã®å‹æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ãŒå®Œå…¨ã«æ©Ÿèƒ½ã—ã¦ã„ãªã„**

### ğŸ“Š è©³ç´°åˆ†æçµæœ

**å…¨ä½“çŠ¶æ³:**

- ç·ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: 2,428
- å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 39/40
- **å‹ä¸€è‡´ç‡: ã‚ãšã‹42.7%** â—

**æœ€æ‚ªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸€è‡´ç‡0%ï¼‰:**

- `æ¶ˆè²»ç¨åˆ¥ï¼ˆä¸¸ï¼‰_ZPR01201.txt`
- `zs65_sss.txt`
- `zs65.txt`
- `æ¶ˆè²»ç¨åˆ¥ï¼ˆå€‹åˆ¥ï¼‰_ZPR01201.txt`

**å‹ä¸ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³:**

1. **REAL â†’ TEXT**: 1,025ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (73.6%)
2. **INTEGER â†’ TEXT**: 111ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (8.0%)
3. **DATETIME â†’ TEXT**: 118ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (8.5%)

**SAPç‰¹æ®Šãƒ‡ãƒ¼ã‚¿æ¤œå‡º:**

- **å¾Œã‚ãƒã‚¤ãƒŠã‚¹å€™è£œ**: **1,392ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰** (å…¨ä¸ä¸€è‡´åˆ†ï¼)
- **é‡è¤‡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å**: 316ç¨®é¡

## ğŸš¨ ç·Šæ€¥å¯¾å¿œå¿…è¦

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: analyzer.pyã®å‹æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’èª¿æŸ»**

S

Read file

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `end_line`: `50`, `start_line`: 0, `relative_path`: `analyzer.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`import os import pandas as pd import sqlite3 from config import DELIMITERS, ENCODINGS, SKIP_EXTENSIONS def infer_sqlite_type(series, column_name): s = series.dropna().astype(str) if len(s) == 0: return "TEXT" # ã‚³ãƒ¼ãƒ‰ç³»ã¯ç„¡æ¡ä»¶ã§TEXT if any(key in column_name.upper() for key in ["CD", "ã‚³ãƒ¼ãƒ‰", "ID", "NO", "ç•ªå·", "æŒ‡å›³", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"]): return "TEXT" # 0ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ··åœ¨ã¯TEXT if all(x.isdigit() for x in s) and any(x.startswith("0") for x in s): return "TEXT" # æ—¥ä»˜åˆ¤å®š date_formats = ["%Y-%m-%d", "%Y/%m/%d", "%Y%m%d", "%Y-%m-%d %H:%M:%S", "%H:%M:%S"] for fmt in date_formats: try: pd.to_datetime(s, format=fmt, errors="raise") return "DATETIME" except Exception: continue try: s.astype(int) return "INTEGER" except Exception: pass try: s.astype(float) return "REAL" except Exception: pass return "TEXT" def detect_delimiter(file_path, encoding, sample_lines=5): """è¤‡æ•°è¡Œã‚’ä½¿ã£ã¦åŒºåˆ‡ã‚Šæ–‡å­—ã‚’æ¨å®šã™ã‚‹""" with open(file_path, "r", encoding=encoding, errors="ignore") as f: lines = [f.readline() for _ in range(sample_lines)] text = "".join(lines) counts = {d: text.count(d) for d in DELIMITERS} return max(counts, key=counts.get) def analyze_files(data_dir, output_file, db_file="master.db"): results = []`

**å‹æ¨å®šãƒ­ã‚¸ãƒƒã‚¯ç™ºè¦‹ï¼** `infer_sqlite_type`é–¢æ•°ã«å•é¡ŒãŒã‚ã‚Šãã†ã§ã™ã€‚

å®Ÿéš›ã®SQLiteãƒ†ãƒ¼ãƒ–ãƒ«ã®å‹ãŒã©ã†ãªã£ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼š

S

Execute shell command

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `command`: `python -c \" import sqlite3 import os from config import OUTPUT_DIR db_file = os.path.join(OUTPUT_DIR, 'master.db') conn = sqlite3.connect(db_file) # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾— cursor = conn.cursor() cursor.execute(\\\"SELECT name FROM sqlite_master WHERE type='table'\\\") tables = cursor.fetchall() print('=== SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ ===') for table in tables: table_name = table[0] print(f'ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}') # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª cursor.execute(f'PRAGMA table_info({table_name})') columns = cursor.fetchall() if table_name != 'column_master': print(' åˆ—æ§‹é€ :') for col in columns[:5]: # æœ€åˆã®5åˆ—ã®ã¿è¡¨ç¤º print(f' {col[1]:20} {col[2]}') if len(columns) > 5: print(f' ... (ä»–{len(columns)-5}åˆ—)') break # 1ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿è©³ç´°è¡¨ç¤º conn.close() \"` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`{"stdout":"=== SQLiteï¿½fï¿½[ï¿½^ï¿½xï¿½[ï¿½X ï¿½eï¿½[ï¿½uï¿½ï¿½ï¿½ê—— ===\nï¿½eï¿½[ï¿½uï¿½ï¿½: column_master\n","return_code":0,"cwd":"C:\\Projects_workspace\\03_python","stderr":""}`

ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã®çŠ¶æ³ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã€å®Ÿéš›ã«analyzeã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

S

Execute shell command

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `command`: `python main.py analyze` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`{"stdout":"ï¿½gï¿½pï¿½ï¿½ï¿½ï¿½DBï¿½tï¿½@ï¿½Cï¿½ï¿½: C:\\Projects_workspace\\03_python\\output\\master.db\nï¿½Ç‚İï¿½ï¿½İï¿½ï¿½s: ï¿½ï¿½ï¿½oï¿½ï¿½ï¿½×iï¿½ï¿½ï¿½j_ZPR01201.txt\nï¿½Ç‚İï¿½ï¿½İï¿½ï¿½s: ï¿½ï¿½ï¿½oï¿½ï¿½ï¿½×iï¿½ï¿½ï¿½ï¿½j_ZPR01201.txt\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½oï¿½Í‚ï¿½ï¿½Ü‚ï¿½ï¿½ï¿½ ï¿½ï¿½ C:\\Projects_workspace\\03_python\\output\\column_mapping_candidates.csv\nSQLiteï¿½É•Û‘ï¿½ï¿½ï¿½ï¿½Ü‚ï¿½ï¿½ï¿½ ï¿½ï¿½ C:\\Projects_workspace\\03_python\\output\\master.db\n","return_code":0,"cwd":"C:\\Projects_workspace\\03_python","stderr":""}`

å®Ÿè¡Œã§ãã¾ã—ãŸï¼ä»Šåº¦ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä¸­èº«ã‚’ç¢ºèªã—ã¾ã™ï¼š

S

Execute shell command

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `command`: `python -c \" import sqlite3 import os from config import OUTPUT_DIR db_file = os.path.join(OUTPUT_DIR, 'master.db') conn = sqlite3.connect(db_file) cursor = conn.cursor() # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ cursor.execute('SELECT name FROM sqlite_master WHERE type=\\\"table\\\"') tables = cursor.fetchall() print('=== ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ ===') for table in tables: table_name = table[0] cursor.execute(f'SELECT COUNT(*) FROM {table_name}') count = cursor.fetchone()[0] print(f'{table_name}: {count:,} ãƒ¬ã‚³ãƒ¼ãƒ‰') # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæœ€åˆã®1ã¤ï¼‰ã‚’èª¿æŸ» cursor.execute('SELECT name FROM sqlite_master WHERE type=\\\"table\\\" AND name != \\\"column_master\\\"') data_tables = cursor.fetchall() if data_tables: first_table = data_tables[0][0] print(f'\\\ === {first_table} ã®æ§‹é€ èª¿æŸ» ===') cursor.execute(f'PRAGMA table_info({first_table})') columns = cursor.fetchall() print('åˆ—æƒ…å ±:') for col in columns[:10]: col_name, col_type = col[1], col[2] print(f' {col_name:30} {col_type}') print(f'\\\ === {first_table} ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ« ===') cursor.execute(f'SELECT * FROM {first_table} LIMIT 3') rows = cursor.fetchall() col_names = [desc[0] for desc in cursor.description] for row in rows: print('ãƒ¬ã‚³ãƒ¼ãƒ‰:') for i, (col_name, value) in enumerate(zip(col_names[:5], row[:5])): print(f' {col_name:20}: {repr(str(value)[:50])}') break conn.close() \"` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`{"stdout":"=== ï¿½eï¿½[ï¿½uï¿½ï¿½ï¿½ê—— ===\ncolumn_master: 2,320 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nGetSekkeiWBSJisseki: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nKANSEI_JISSEKI: 141 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nKOUSU_JISSEKI: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nKOUTEI_JISSEKI: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nPP_DL_CSV_SEISAN_YOTEI: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nPP_DL_CSV_ZTBP110: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nPP_SUMMARY_ZTBP080_KOJOZISSEKI_D_0: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nSASIZU_JISSEKI: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nV_SAGYO_TIME: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzf26: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzm114: 30 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZM122_P100_1220: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZM122_P100_1240: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzm21: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZM29: 264 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzm37: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzm87n: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzp02: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZP128_P100: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZP128_P300: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZP138: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzp160: 428 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZP170: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZP173: 747 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZP173_MEISAI: 878 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzp35: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZP51N: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzp58: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzp70: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZPF01802_ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½: 731 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzs191: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzs45: 19 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZS58MONTH: 563 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nZS61KDAY: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzs65: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nzs65_sss: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nï¿½ï¿½ï¿½oï¿½ï¿½ï¿½ï¿½_ï¿½ï¿½ï¿½__ZPR01201: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nï¿½ï¿½ï¿½oï¿½ï¿½ï¿½ï¿½_ï¿½ï¿½ï¿½ï¿½__ZPR01201: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\nï¿½ï¿½ï¿½ï¿½ï¿½Ë—ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ê——: 1,000 ï¿½ï¿½ï¿½Rï¿½[ï¿½h\n\\n=== GetSekkeiWBSJisseki ï¿½Ì\\ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ===\nï¿½ï¿½ï¿½ï¿½:\n DSGN_MRP TEXT\n DSGN_WBS_NO TEXT\n DSGN_NTWK_NO TEXT\n CM_CODE TEXT\n ITM_CD TEXT\n MODEL TEXT\n PRD_NAME TEXT\n SALE_NUM TEXT\n PRD_AMOUNT TEXT\n matCost_de TEXT\n\\n=== GetSekkeiWBSJisseki ï¿½Ìï¿½ï¿½fï¿½[ï¿½^ï¿½Tï¿½ï¿½ï¿½vï¿½ï¿½ ===\nï¿½ï¿½ï¿½Rï¿½[ï¿½h:\n DSGN_MRP : 'CW0'\n DSGN_WBS_NO : 'C-0080664-20-01'\n DSGN_NTWK_NO : '40047235'\n CM_CODE : 'C'\n ITM_CD : 'A00A0580700'\n","return_code":0,"cwd":"C:\\Projects_workspace\\03_python","stderr":""}`

**å•é¡Œåˆ¤æ˜ï¼ï¼** ğŸ’¡

## ğŸ¯ æ ¹æœ¬åŸå› ç¢ºå®š

**SQLiteãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ã¦ã®åˆ—ãŒ `TEXT` å‹ã§ä½œæˆã•ã‚Œã¦ã„ã‚‹ï¼**

ã“ã‚ŒãŒ `Actual_Type` ãŒå…¨ã¦ `TEXT` ã«ãªã£ã¦ã„ã‚‹åŸå› ã§ã™ã€‚

`GetSekkeiWBSJisseki` ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¾‹ï¼š

- `SALE_NUM` (å£²ä¸Šæ•°é‡) â†’ æœ¬æ¥ã¯ `INTEGER` or `REAL` ã§ã‚ã‚‹ã¹ã
- `PRD_AMOUNT` (è£½å“æ•°é‡) â†’ æœ¬æ¥ã¯ `INTEGER` or `REAL` ã§ã‚ã‚‹ã¹ã
- `matCost_de` (ææ–™è²») â†’ æœ¬æ¥ã¯ `REAL` ã§ã‚ã‚‹ã¹ã

ã—ã‹ã—ã€**å…¨ã¦ `TEXT` å‹ã§ä½œæˆ**ã•ã‚Œã¦ã„ã¾ã™ã€‚

### ğŸ” å•é¡Œç®‡æ‰€ã®ç‰¹å®š

å•é¡Œã¯**SQLiteãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚ã®å‹æŒ‡å®š**ã«ã‚ã‚Šã¾ã™ã€‚ç¾åœ¨ã®å®Ÿè£…ã§ã¯ï¼š

1. **`infer_sqlite_type`ã§å‹æ¨å®š** â†’ æ­£ã—ãå‹•ä½œ
2. **SQLiteãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚** â†’ **å…¨ã¦ `TEXT` ã§ä½œæˆã•ã‚Œã¦ã„ã‚‹** âŒ

analyzer.pyã®ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆéƒ¨åˆ†ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼š

S

Read file

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `end_line`: `120`, `start_line`: 50, `relative_path`: `analyzer.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`results = [] for file_name in os.listdir(data_dir): file_path = os.path.join(data_dir, file_name) if not os.path.isfile(file_path): continue if any(file_name.lower().endswith(ext) for ext in SKIP_EXTENSIONS): continue # Excel if file_name.lower().endswith((".xls", ".xlsx")): try: df = pd.read_excel(file_path, nrows=200, dtype=str) for col in df.columns: sqlite_type = infer_sqlite_type(df[col], col) results.append({ "File": file_name, "Column": col, "Inferred_Type": sqlite_type, "Encoding": "excel", "Delimiter": None }) continue except Exception as e: print(f"èª­ã¿è¾¼ã¿å¤±æ•—(Excel): {file_name}, {e}") continue # ãƒ†ã‚­ã‚¹ãƒˆ/CSV success = False for enc in ENCODINGS: try: delimiter = detect_delimiter(file_path, enc) df = pd.read_csv(file_path, delimiter=delimiter, dtype=str, nrows=200, encoding=enc, engine="python") for col in df.columns: sqlite_type = infer_sqlite_type(df[col], col) results.append({ "File": file_name, "Column": col, "Inferred_Type": sqlite_type, "Encoding": enc, "Delimiter": delimiter }) success = True break except Exception: continue if not success: print(f"èª­ã¿è¾¼ã¿å¤±æ•—: {file_name}") # CSVä¿å­˜ pd.DataFrame(results).to_csv(output_file, index=False, encoding="utf-8-sig") print(f"åˆ—å€™è£œã‚’å‡ºåŠ›ã—ã¾ã—ãŸ â†’ {output_file}") # SQLiteã«ä¿å­˜ conn = sqlite3.connect(db_file) cur = conn.cursor() cur.execute(""" CREATE TABLE IF NOT EXISTS column_master ( file_name TEXT, column_name TEXT, data_type TEXT, encoding TEXT, delimiter TEXT, PRIMARY KEY (file_name, column_name) ) """) for row in results:`

S

Read file

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `end_line`: `200`, `start_line`: 120, `relative_path`: `analyzer.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`for row in results: cur.execute(""" INSERT INTO column_master (file_name, column_name, data_type, encoding, delimiter) VALUES (?, ?, ?, ?, ?) ON CONFLICT(file_name, column_name) DO UPDATE SET data_type=excluded.data_type, encoding=excluded.encoding, delimiter=excluded.delimiter """, (row["File"], row["Column"], row["Inferred_Type"], row.get("Encoding"), row.get("Delimiter"))) conn.commit() conn.close() print(f"SQLiteã«ä¿å­˜ã—ã¾ã—ãŸ â†’ {db_file}")`

ã“ã®éƒ¨åˆ†ã¯**ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆcolumn_masterï¼‰ã®ä¿å­˜**ã§ã™ã­ã€‚å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã¯åˆ¥ã®å ´æ‰€ã«ã‚ã‚Šãã†ã§ã™ã€‚

`loader.py`ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

S

Read file

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `end_line`: `50`, `start_line`: 0, `relative_path`: `loader.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`import os import pandas as pd import sqlite3 from typing import Optional, Tuple, Dict, List from config import DATA_DIR, DB_FILE, OUTPUT_DIR, DELIMITERS, ENCODINGS, SKIP_EXTENSIONS def detect_delimiter_simple(file_path: str, encoding: str) -> str: """ã‚·ãƒ³ãƒ—ãƒ«ãªåŒºåˆ‡ã‚Šæ–‡å­—æ¤œå‡º""" try: with open(file_path, 'r', encoding=encoding, errors='ignore') as f: # æœ€åˆã®æ•°è¡Œã‚’èª­ã‚€ sample_text = "" for i in range(5): line = f.readline() if not line: break sample_text += line # å„åŒºåˆ‡ã‚Šæ–‡å­—ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ delimiter_counts = {} for delimiter in ['\t', ',', '|', ';']: # ã‚ˆãä½¿ã‚ã‚Œã‚‹åŒºåˆ‡ã‚Šæ–‡å­— count = sample_text.count(delimiter) if count > 0: delimiter_counts[delimiter] = count # æœ€ã‚‚å¤šã„åŒºåˆ‡ã‚Šæ–‡å­—ã‚’è¿”ã™ï¼ˆãªã‘ã‚Œã°ã‚¿ãƒ–ï¼‰ if delimiter_counts: return max(delimiter_counts.keys(), key=delimiter_counts.get) else: return '\t' # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ except Exception: return '\t' def safe_read_csv(file_path: str, encoding: str, delimiter: str) -> Optional[pd.DataFrame]: """å®‰å…¨ãªCSVèª­ã¿è¾¼ã¿ï¼ˆpandas ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•ã‚ãšå‹•ä½œï¼‰""" try: # åŸºæœ¬çš„ãªå¼•æ•°ã®ã¿ä½¿ç”¨ df = pd.read_csv( file_path, sep=delimiter, # delimiterã®ä»£ã‚ã‚Šã«sepã‚’ä½¿ç”¨ dtype=str, encoding=encoding, engine='python', nrows=1000, # ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ na_filter=False # NaNå¤‰æ›ã‚’ç„¡åŠ¹åŒ– ) return df except Exception: return None`

S

Find symbol

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `name_path`: `create_table`, `relative_path`: `loader.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`[]`

S

Search for pattern

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `relative_path`: `.`, `substring_pattern`: `CREATE TABLE.*IF NOT EXISTS`, `paths_include_glob`: `*.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`{"analyzer.py": [" > 111: CREATE TABLE IF NOT EXISTS column_master ("], "db.py": [" > 8: CREATE TABLE IF NOT EXISTS column_master ("], "init_prod.py": [" > 10: CREATE TABLE IF NOT EXISTS column_master ("], "master_manager.py": [" > 12: CREATE TABLE IF NOT EXISTS column_master ("]}`

S

Search for pattern

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `relative_path`: `.`, `substring_pattern`: `to_sql`, `paths_include_glob`: `*.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`{"loader.py": [" > 183: df.to_sql(table_name, conn, if_exists='replace', index=False)"], "master_manager.py": [" > 29: candidates.to_sql(\"column_master\", conn, if_exists=\"append\", index=False)"]}`

**ç™ºè¦‹ï¼** `loader.py`ã®183è¡Œç›®ã§`to_sql`ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ã“ã‚ŒãŒå•é¡Œã®åŸå› ã§ã™ï¼

S

Read file

ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

``{ `end_line`: `190`, `start_line`: 175, `relative_path`: `loader.py` }``

ãƒ¬ã‚¹ãƒãƒ³ã‚¹

`continue # ãƒ†ãƒ¼ãƒ–ãƒ«åç”Ÿæˆ table_name = sanitize_table_name(file_name) # SQLiteã«ä¿å­˜ try: df.to_sql(table_name, conn, if_exists='replace', index=False) print(f"SQLiteä¿å­˜å®Œäº†: {table_name}") except Exception as e: print(f"SQLiteä¿å­˜å¤±æ•—: {e}") error_count += 1 continue # ã‚¹ã‚­ãƒ¼ãƒæ¯”è¼ƒ actual_schema = get_table_info(conn, table_name)`

**å•é¡Œç™ºè¦‹ï¼** ğŸ’¡

## ğŸ¯ æ ¹æœ¬åŸå› : `DataFrame.to_sql()`ã®å‹æ¨å®šå•é¡Œ

**`df.to_sql(table_name, conn, if_exists='replace', index=False)`**

ã“ã®ã‚³ãƒ¼ãƒ‰ãŒå•é¡Œã§ã™ï¼`DataFrame.to_sql()`ã¯ï¼š

1. **å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒ`str`å‹** (`dtype=str`ã§èª­ã¿è¾¼ã¿)
2. **SQLiteã«è‡ªå‹•å¤‰æ›æ™‚** â†’ å…¨ã¦`TEXT`å‹ã«ãªã‚‹
3. **å‹æ¨å®šãƒ­ã‚¸ãƒƒã‚¯** (`infer_sqlite_type`) ã¯å‹•ããŒã€**SQLiteä½œæˆæ™‚ã«åæ˜ ã•ã‚Œãªã„**

## ğŸ”§ T001ã®è§£æ±ºæ–¹é‡

**DataFrameã®å‹å¤‰æ›ã‚’SQLiteä¿å­˜å‰ã«å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™**