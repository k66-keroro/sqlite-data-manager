# SQLite Data Manager - ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

## ğŸš¨ ç·Šæ€¥æ™‚å¯¾å¿œ

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç ´ææ™‚ã®å¯¾å¿œ
1. **å³åº§ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©æ—§**
   ```python
   from t002_backup_system import BackupManager
   backup_manager = BackupManager()
   
   # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ç¢ºèª
   backups = backup_manager.list_backups()
   latest_backup = backups[0]  # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
   
   # å¾©æ—§å®Ÿè¡Œ
   success = backup_manager.restore_backup(latest_backup['backup_id'], 'output/master.db')
   ```

2. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯**
   ```sql
   PRAGMA integrity_check;
   PRAGMA foreign_key_check;
   ```

### å‡¦ç†ãŒåœæ­¢ã—ãŸå ´åˆ
1. **ãƒ—ãƒ­ã‚»ã‚¹å¼·åˆ¶çµ‚äº†**
   - Ctrl+C ã§ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†
   - ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§Pythonãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†

2. **ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤**
   ```bash
   # SQLiteãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
   del output\master.db-wal
   del output\master.db-shm
   ```

## âŒ ã‚¨ãƒ©ãƒ¼åˆ¥å¯¾å‡¦æ³•

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ã‚¨ãƒ©ãƒ¼

#### `database is locked`
**åŸå› **: ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ä¸­
**å¯¾å‡¦æ³•**:
1. DB Browser for SQLiteã‚’é–‰ã˜ã‚‹
2. ä»–ã®Pythonãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†
3. ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
   ```python
   import os
   lock_files = ['output/master.db-wal', 'output/master.db-shm']
   for lock_file in lock_files:
       if os.path.exists(lock_file):
           os.remove(lock_file)
   ```

#### `no such table: table_name`
**åŸå› **: ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„
**å¯¾å‡¦æ³•**:
```python
# ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table_name,))
if not cursor.fetchone():
    print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    # å¿…è¦ã«å¿œã˜ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
```

#### `no such column: column_name`
**åŸå› **: ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„
**å¯¾å‡¦æ³•**:
```python
# ã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèª
cursor.execute(f"PRAGMA table_info({table_name})")
columns = [row[1] for row in cursor.fetchall()]
if column_name not in columns:
    print(f"ã‚«ãƒ©ãƒ  {column_name} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    print(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {columns}")
```

### ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼

#### `UnicodeDecodeError`
**åŸå› **: æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸ä¸€è‡´
**å¯¾å‡¦æ³•**:
```python
# è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
encodings = ['utf-8', 'cp932', 'shift_jis', 'iso-2022-jp']
for encoding in encodings:
    try:
        df = pd.read_csv(file_path, encoding=encoding)
        print(f"æˆåŠŸ: {encoding}")
        break
    except UnicodeDecodeError:
        continue
```

#### `ParserError: Error tokenizing data`
**åŸå› **: CSVå½¢å¼ã®å•é¡Œï¼ˆåŒºåˆ‡ã‚Šæ–‡å­—ã€å¼•ç”¨ç¬¦ãªã©ï¼‰
**å¯¾å‡¦æ³•**:
```python
# åŒºåˆ‡ã‚Šæ–‡å­—ã‚’è‡ªå‹•æ¤œå‡º
import csv
with open(file_path, 'r', encoding='cp932') as f:
    sample = f.read(1024)
    sniffer = csv.Sniffer()
    delimiter = sniffer.sniff(sample).delimiter
    
df = pd.read_csv(file_path, delimiter=delimiter, encoding='cp932', engine='python')
```

### ãƒ¡ãƒ¢ãƒªé–¢é€£ã‚¨ãƒ©ãƒ¼

#### `MemoryError`
**åŸå› **: ãƒ¡ãƒ¢ãƒªä¸è¶³
**å¯¾å‡¦æ³•**:
```python
# ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã§å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
chunk_size = 1000
for chunk in pd.read_csv(file_path, chunksize=chunk_size):
    process_chunk(chunk)
    
# ãƒ¡ãƒ¢ãƒªè§£æ”¾
import gc
gc.collect()
```

#### `OverflowError`
**åŸå› **: æ•°å€¤ãŒç¯„å›²ã‚’è¶…é
**å¯¾å‡¦æ³•**:
```python
# ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
df = pd.read_csv(file_path, dtype={'large_number_field': 'str'})

# å¿…è¦ã«å¿œã˜ã¦å¾Œã§å¤‰æ›
df['large_number_field'] = pd.to_numeric(df['large_number_field'], errors='coerce')
```

## ğŸ” è¨ºæ–­ãƒ„ãƒ¼ãƒ«

### ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
```python
def system_health_check():
    """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    import psutil
    import sqlite3
    
    print("=== ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ ===")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    memory = psutil.virtual_memory()
    print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent}%")
    print(f"åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {memory.available / 1024**3:.2f} GB")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
    disk = psutil.disk_usage('.')
    print(f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡: {disk.percent}%")
    print(f"ç©ºãå®¹é‡: {disk.free / 1024**3:.2f} GB")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹
    try:
        conn = sqlite3.connect('output/master.db')
        cursor = conn.cursor()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
        cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
        db_size = cursor.fetchone()[0]
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {db_size / 1024**2:.2f} MB")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°
        cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table'")
        table_count = cursor.fetchone()[0]
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {table_count}")
        
        conn.close()
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š: OK")
        
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

system_health_check()
```

### ãƒ­ã‚°åˆ†æãƒ„ãƒ¼ãƒ«
```python
def analyze_logs():
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
    import json
    from pathlib import Path
    
    log_files = list(Path('.').glob('*.log'))
    if not log_files:
        print("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    for log_file in log_files:
        print(f"\n=== {log_file} ===")
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        error_count = sum(1 for line in lines if 'ERROR' in line)
        warning_count = sum(1 for line in lines if 'WARNING' in line)
        
        print(f"ç·è¡Œæ•°: {len(lines)}")
        print(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
        print(f"è­¦å‘Š: {warning_count}ä»¶")
        
        # æœ€æ–°ã®ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º
        recent_errors = [line for line in lines[-100:] if 'ERROR' in line]
        if recent_errors:
            print("æœ€æ–°ã®ã‚¨ãƒ©ãƒ¼:")
            for error in recent_errors[-3:]:
                print(f"  {error.strip()}")

analyze_logs()
```

## ğŸ› ï¸ ä¿®å¾©ãƒ„ãƒ¼ãƒ«

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿®å¾©
```python
def repair_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿®å¾©"""
    import sqlite3
    import shutil
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    shutil.copy2('output/master.db', 'output/master_backup.db')
    
    conn = sqlite3.connect('output/master.db')
    cursor = conn.cursor()
    
    try:
        # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        cursor.execute("PRAGMA integrity_check")
        result = cursor.fetchone()[0]
        
        if result != 'ok':
            print(f"æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å¤±æ•—: {result}")
            
            # ä¿®å¾©è©¦è¡Œ
            cursor.execute("PRAGMA quick_check")
            cursor.execute("REINDEX")
            cursor.execute("VACUUM")
            
            print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿®å¾©å®Œäº†")
        else:
            print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯æ­£å¸¸ã§ã™")
            
    except Exception as e:
        print(f"ä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
        shutil.copy2('output/master_backup.db', 'output/master.db')
        print("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã—ã¾ã—ãŸ")
    
    finally:
        conn.close()

repair_database()
```

### å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
```python
def cleanup_orphaned_data():
    """å­¤ç«‹ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    conn = sqlite3.connect('output/master.db')
    cursor = conn.cursor()
    
    try:
        # column_masterã«å­˜åœ¨ã—ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name != 'column_master'")
        existing_tables = {row[0] for row in cursor.fetchall()}
        
        cursor.execute("SELECT DISTINCT file_name FROM column_master")
        registered_files = {row[0].replace('.txt', '').replace('.csv', '') for row in cursor.fetchall()}
        
        orphaned_tables = existing_tables - registered_files
        
        if orphaned_tables:
            print(f"å­¤ç«‹ãƒ†ãƒ¼ãƒ–ãƒ«ç™ºè¦‹: {orphaned_tables}")
            for table in orphaned_tables:
                cursor.execute(f"DROP TABLE IF EXISTS {table}")
                print(f"å‰Šé™¤: {table}")
        
        conn.commit()
        print("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        
    except Exception as e:
        print(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        conn.rollback()
    
    finally:
        conn.close()

cleanup_orphaned_data()
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ

### å‡¦ç†é€Ÿåº¦ãŒé…ã„å ´åˆ

#### åŸå› åˆ†æ
```python
import time
import cProfile

def profile_function(func, *args, **kwargs):
    """é–¢æ•°ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    profiler = cProfile.Profile()
    profiler.enable()
    
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    
    profiler.disable()
    profiler.print_stats(sort='cumulative')
    
    print(f"å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
    return result
```

#### æœ€é©åŒ–æ‰‹æ³•
1. **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ **
   ```sql
   CREATE INDEX idx_file_column ON column_master(file_name, column_name);
   CREATE INDEX idx_data_type ON column_master(data_type);
   ```

2. **ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´**
   ```python
   # å°ã•ãªãƒãƒƒãƒã§å‡¦ç†
   batch_size = 50
   for i in range(0, len(modifications), batch_size):
       batch_mods = modifications[i:i+batch_size]
       process_batch(batch_mods)
   ```

3. **ä¸¦åˆ—å‡¦ç†**
   ```python
   from concurrent.futures import ThreadPoolExecutor
   
   with ThreadPoolExecutor(max_workers=4) as executor:
       futures = [executor.submit(process_file, file) for file in files]
       results = [future.result() for future in futures]
   ```

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„å ´åˆ

#### ãƒ¡ãƒ¢ãƒªç›£è¦–
```python
import tracemalloc

def monitor_memory():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–"""
    tracemalloc.start()
    
    # å‡¦ç†å®Ÿè¡Œ
    process_data()
    
    current, peak = tracemalloc.get_traced_memory()
    print(f"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {current / 1024**2:.2f} MB")
    print(f"ãƒ”ãƒ¼ã‚¯æ™‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {peak / 1024**2:.2f} MB")
    
    tracemalloc.stop()
```

#### ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
```python
def optimize_memory():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–"""
    import gc
    
    # ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤
    del large_dataframe
    
    # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    gc.collect()
    
    # DataFrameã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›
    df = df.copy()  # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è§£æ¶ˆ
    
    # ã‚«ãƒ†ã‚´ãƒªå‹ã‚’ä½¿ç”¨
    df['category_column'] = df['category_column'].astype('category')
```

## ğŸ”§ è¨­å®šå•é¡Œ

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼

#### pattern_rules_data.json ã®ä¿®å¾©
```python
def repair_pattern_rules():
    """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«è¨­å®šã‚’ä¿®å¾©"""
    import json
    
    default_rules = {
        "sap_patterns": {
            "trailing_minus": r"^(\d+\.?\d*)-$",
            "zero_padding": r"^0+(\d+)$",
            "decimal_comma": r"^(\d+),(\d+)$"
        },
        "datetime_formats": [
            "%Y%m%d",
            "%Y/%m/%d",
            "%Y-%m-%d"
        ],
        "unregistered_files": {}
    }
    
    try:
        with open('pattern_rules_data.json', 'r', encoding='utf-8') as f:
            current_rules = json.load(f)
        
        # ä¸è¶³ã—ã¦ã„ã‚‹è¨­å®šã‚’è£œå®Œ
        for key, value in default_rules.items():
            if key not in current_rules:
                current_rules[key] = value
                print(f"è¿½åŠ : {key}")
        
        # ä¿®å¾©æ¸ˆã¿è¨­å®šã‚’ä¿å­˜
        with open('pattern_rules_data.json', 'w', encoding='utf-8') as f:
            json.dump(current_rules, f, ensure_ascii=False, indent=2)
        
        print("ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«è¨­å®šä¿®å¾©å®Œäº†")
        
    except Exception as e:
        print(f"è¨­å®šä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ä¸Šæ›¸ã
        with open('pattern_rules_data.json', 'w', encoding='utf-8') as f:
            json.dump(default_rules, f, ensure_ascii=False, indent=2)
        print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å¾©æ—§ã—ã¾ã—ãŸ")

repair_pattern_rules()
```

## ğŸ“ ã‚µãƒãƒ¼ãƒˆæƒ…å ±

### ãƒ­ã‚°åé›†
å•é¡Œå ±å‘Šæ™‚ã¯ä»¥ä¸‹ã®æƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š

```python
def collect_support_info():
    """ã‚µãƒãƒ¼ãƒˆç”¨æƒ…å ±ã‚’åé›†"""
    import platform
    import sys
    import sqlite3
    
    info = {
        "system": {
            "platform": platform.platform(),
            "python_version": sys.version,
            "architecture": platform.architecture()
        },
        "database": {},
        "files": {},
        "errors": []
    }
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±
    try:
        conn = sqlite3.connect('output/master.db')
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table'")
        info["database"]["table_count"] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM column_master")
        info["database"]["field_count"] = cursor.fetchone()[0]
        
        conn.close()
    except Exception as e:
        info["errors"].append(f"Database error: {e}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
    import os
    for file_name in ['pattern_rules_data.json', 'rule_templates.json']:
        info["files"][file_name] = os.path.exists(file_name)
    
    # æƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    import json
    with open('support_info.json', 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    
    print("ã‚µãƒãƒ¼ãƒˆæƒ…å ±ã‚’ support_info.json ã«ä¿å­˜ã—ã¾ã—ãŸ")

collect_support_info()
```

### ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

#### Q: å‡¦ç†ãŒé€”ä¸­ã§æ­¢ã¾ã£ã¦ã—ã¾ã„ã¾ã™
A: ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆ8GBä»¥ä¸Šæ¨å¥¨ï¼‰
- ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºã®3å€ä»¥ä¸Šï¼‰
- ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚‹DBãƒ­ãƒƒã‚¯

#### Q: æ–‡å­—åŒ–ã‘ãŒç™ºç”Ÿã—ã¾ã™
A: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šã‚’ç¢ºèªï¼š
```python
# æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
df = pd.read_csv(file_path, encoding='cp932')
```

#### Q: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©æ—§ã§ãã¾ã›ã‚“
A: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ã‚’ç¢ºèªï¼š
```python
backup_manager.verify_backup_integrity(backup_info)
```

### ç·Šæ€¥é€£çµ¡å…ˆ
- GitHub Issues: https://github.com/k66-keroro/sqlite-data-manager/issues
- æŠ€è¡“ã‚µãƒãƒ¼ãƒˆ: é–‹ç™ºãƒãƒ¼ãƒ ã¾ã§ãŠå•ã„åˆã‚ã›ãã ã•ã„

---

ã“ã®ã‚¬ã‚¤ãƒ‰ã§è§£æ±ºã—ãªã„å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€support_info.json ã¨å…±ã«å•é¡Œã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚